{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c87b6eb",
   "metadata": {},
   "source": [
    "# Gemma Model Quick Start Guide\n",
    "\n",
    "This notebook demonstrates how to use the Gemma 1.1 Instruct 2B model with Keras Hub.\n",
    "\n",
    "**Model Details:**\n",
    "- **Family**: Gemma (Google)\n",
    "- **Variant**: Gemma 1.1 Instruct 2B EN\n",
    "- **Parameters**: 2 billion\n",
    "- **Type**: Instruction-tuned text generation model\n",
    "- **Languages**: English\n",
    "- **Backends**: JAX, TensorFlow, PyTorch (via Keras 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81c956",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install required packages for Gemma model usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5aa490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Keras Hub and Keras 3\n",
    "!pip install -q -U keras-hub\n",
    "!pip install -q -U keras\n",
    "\n",
    "# Optional: Install JAX backend (recommended for Gemma)\n",
    "!pip install -q jax[cpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f7a20",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Configure Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set Keras backend (choose: 'jax', 'tensorflow', or 'torch')\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "\n",
    "import keras\n",
    "import keras_hub\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Keras Hub version: {keras_hub.__version__}\")\n",
    "print(f\"Using backend: {keras.backend.backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477cf2f5",
   "metadata": {},
   "source": [
    "## 3. Load Gemma Model\n",
    "\n",
    "Load the Gemma 1.1 Instruct 2B model from Keras Hub presets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0940b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Gemma 1.1 Instruct 2B model\n",
    "# Note: First time will download ~5GB of model weights\n",
    "print(\"Loading Gemma 1.1 Instruct 2B model...\")\n",
    "print(\"This may take a few minutes on first run (downloading ~5GB)\\n\")\n",
    "\n",
    "gemma_lm = keras_hub.models.GemmaCausalLM.from_preset(\"gemma_1.1_instruct_2b_en\")\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8e4ae",
   "metadata": {},
   "source": [
    "## 4. Basic Text Generation\n",
    "\n",
    "Use the `generate()` method for simple text completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text generation\n",
    "prompt = \"Keras is a\"\n",
    "output = gemma_lm.generate(prompt, max_length=30)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acadec54",
   "metadata": {},
   "source": [
    "## 5. Batch Generation\n",
    "\n",
    "Generate text for multiple prompts simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch text generation\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"Machine learning can help solve\",\n",
    "    \"Deep learning models are\"\n",
    "]\n",
    "\n",
    "outputs = gemma_lm.generate(prompts, max_length=50)\n",
    "\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798d95b",
   "metadata": {},
   "source": [
    "## 6. Custom Sampling Strategies\n",
    "\n",
    "Configure different sampling methods for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb080b68",
   "metadata": {},
   "source": [
    "### Top-K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with Top-K sampler\n",
    "gemma_lm.compile(sampler=\"top_k\")\n",
    "\n",
    "prompt = \"The most important aspect of machine learning is\"\n",
    "output = gemma_lm.generate(prompt, max_length=50)\n",
    "\n",
    "print(f\"Top-K Sampling:\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbce1c",
   "metadata": {},
   "source": [
    "### Beam Search Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with Beam sampler\n",
    "gemma_lm.compile(sampler=keras_hub.samplers.BeamSampler(num_beams=2))\n",
    "\n",
    "prompt = \"Explain quantum computing in simple terms:\"\n",
    "output = gemma_lm.generate(prompt, max_length=100)\n",
    "\n",
    "print(f\"Beam Search (2 beams):\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec016a6d",
   "metadata": {},
   "source": [
    "## 7. Question Answering\n",
    "\n",
    "Use Gemma for question-answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to default sampler\n",
    "gemma_lm = keras_hub.models.GemmaCausalLM.from_preset(\"gemma_1.1_instruct_2b_en\")\n",
    "\n",
    "# Question answering examples\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain what a neural network is in one sentence.\",\n",
    "    \"How does gradient descent work?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = gemma_lm.generate(question, max_length=100)\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64c2af",
   "metadata": {},
   "source": [
    "## 8. Text Summarization\n",
    "\n",
    "Generate summaries of longer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarization\n",
    "text_to_summarize = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, \n",
    "as opposed to natural intelligence displayed by animals including humans. \n",
    "AI research has been defined as the field of study of intelligent agents, \n",
    "which refers to any system that perceives its environment and takes actions \n",
    "that maximize its chance of achieving its goals.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Summarize the following text in one sentence:\\n{text_to_summarize}\\n\\nSummary:\"\n",
    "summary = gemma_lm.generate(prompt, max_length=100)\n",
    "\n",
    "print(f\"Original text: {text_to_summarize}\")\n",
    "print(f\"\\nSummary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3bdceb",
   "metadata": {},
   "source": [
    "## 9. Code Generation\n",
    "\n",
    "Generate code snippets based on natural language descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation example\n",
    "code_prompts = [\n",
    "    \"Write a Python function to calculate the factorial of a number:\",\n",
    "    \"Create a function that sorts a list in Python:\",\n",
    "    \"Write code to read a CSV file using pandas:\"\n",
    "]\n",
    "\n",
    "for prompt in code_prompts:\n",
    "    code = gemma_lm.generate(prompt, max_length=150)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated code:\\n{code}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3f3ff",
   "metadata": {},
   "source": [
    "## 10. Advanced: Low-Level Token Generation\n",
    "\n",
    "Work directly with token IDs without preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e67bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate using token IDs directly\n",
    "prompt = {\n",
    "    # Token IDs: start token followed by \"Keras is\"\n",
    "    \"token_ids\": np.array([[2, 214064, 603, 0, 0, 0, 0]] * 2),\n",
    "    # Padding mask: 1 for real tokens, 0 for padding\n",
    "    \"padding_mask\": np.array([[1, 1, 1, 0, 0, 0, 0]] * 2),\n",
    "}\n",
    "\n",
    "gemma_lm_no_preproc = keras_hub.models.GemmaCausalLM.from_preset(\n",
    "    \"gemma_1.1_instruct_2b_en\",\n",
    "    preprocessor=None,\n",
    ")\n",
    "\n",
    "output = gemma_lm_no_preproc.generate(prompt)\n",
    "print(f\"Token-based generation output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e8822",
   "metadata": {},
   "source": [
    "## 11. Fine-tuning Example (Single Batch)\n",
    "\n",
    "Demonstrate how to fine-tune the model on custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db566d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning on a single batch\n",
    "features = [\n",
    "    \"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\"\n",
    "]\n",
    "\n",
    "# Create a fresh model instance for training\n",
    "gemma_lm_train = keras_hub.models.GemmaCausalLM.from_preset(\"gemma_1.1_instruct_2b_en\")\n",
    "\n",
    "# Note: This is a simple example. In practice, you'd need more data and epochs\n",
    "print(\"Fine-tuning on sample batch...\")\n",
    "gemma_lm_train.fit(x=features, batch_size=2, epochs=1)\n",
    "print(\"✓ Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963bcde1",
   "metadata": {},
   "source": [
    "## 12. Model Information and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbeeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model configuration\n",
    "print(\"Gemma Model Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vocabulary size: {gemma_lm.preprocessor.tokenizer.vocabulary_size()}\")\n",
    "print(f\"\\nModel summary:\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035cc07",
   "metadata": {},
   "source": [
    "## 13. Performance Tips\n",
    "\n",
    "**Optimization strategies:**\n",
    "\n",
    "1. **Choose the right backend**: JAX is recommended for Gemma models\n",
    "2. **Batch processing**: Process multiple prompts together for efficiency\n",
    "3. **Adjust max_length**: Shorter sequences generate faster\n",
    "4. **Use appropriate sampling**: Greedy is fastest, beam search most accurate\n",
    "5. **GPU acceleration**: Use GPU/TPU for production workloads\n",
    "\n",
    "**Memory considerations:**\n",
    "- Gemma 2B requires ~8GB RAM minimum\n",
    "- Gemma 7B requires ~28GB RAM minimum\n",
    "- Use smaller batch sizes if memory-constrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbb4f8",
   "metadata": {},
   "source": [
    "## 14. Common Use Cases\n",
    "\n",
    "**Gemma models excel at:**\n",
    "- ✓ Question answering\n",
    "- ✓ Text summarization\n",
    "- ✓ Code generation\n",
    "- ✓ Chatbots and conversational AI\n",
    "- ✓ Content creation\n",
    "- ✓ Language translation\n",
    "- ✓ Text classification\n",
    "- ✓ Named entity recognition\n",
    "\n",
    "**Limitations:**\n",
    "- ⚠ May generate factually incorrect information\n",
    "- ⚠ Limited to English language\n",
    "- ⚠ Context window limitations\n",
    "- ⚠ May reflect biases from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394acfd",
   "metadata": {},
   "source": [
    "## 15. Next Steps\n",
    "\n",
    "**Explore further:**\n",
    "1. Try the Gemma 7B model for better performance\n",
    "2. Experiment with different sampling strategies\n",
    "3. Fine-tune on domain-specific data\n",
    "4. Build a chatbot or Q&A system\n",
    "5. Integrate with LangChain for advanced applications\n",
    "6. Deploy to production with optimizations\n",
    "\n",
    "**Resources:**\n",
    "- [Gemma on Kaggle](https://www.kaggle.com/models/google/gemma)\n",
    "- [Keras Hub Documentation](https://keras.io/keras_hub/)\n",
    "- [Gemma Model Card](https://ai.google.dev/gemma/docs)\n",
    "- [Responsible AI Toolkit](https://ai.google.dev/responsible)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
